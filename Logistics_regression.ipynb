{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Theoretical questions"
      ],
      "metadata": {
        "id": "iYQO1f7i0C9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "- Logistic Regression is used for binary or multiclass classification problems. Unlike Linear Regression, which predicts continuous values, Logistic Regression predicts probabilities using the sigmoid function, and outputs class labels (0 or 1)."
      ],
      "metadata": {
        "id": "GVmDLcpp0fJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the mathematical equation of Logistic Regression.\n",
        "\n",
        "- h\n",
        "Œ∏(x)=\n",
        "1+e\n",
        "‚àíŒ∏\n",
        "T\n",
        " x"
      ],
      "metadata": {
        "id": "Qt4KrQye0te7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "- The sigmoid function maps any real-valued number to a value between 0 and 1, which can be interpreted as a probability."
      ],
      "metadata": {
        "id": "71-GIDvX0tpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression.\n",
        "- The cost function is:\n",
        "\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "=\n",
        "‚àí\n",
        "1\n",
        "ùëö\n",
        "‚àë\n",
        "[\n",
        "ùë¶\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "‚Ñé\n",
        "ùúÉ\n",
        "(\n",
        "ùë•\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùë¶\n",
        ")\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "‚Ñé\n",
        "ùúÉ\n",
        "(\n",
        "ùë•\n",
        ")\n",
        ")\n",
        "]\n",
        "J(Œ∏)=‚àí\n",
        "m\n",
        "1 ‚àë[ylog(h\n",
        "Œ∏\n",
        "(x))+(1‚àíy)log(1‚àíh  (x))]"
      ],
      "metadata": {
        "id": "3UdQJueU0xwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed.\n",
        "- Regularization adds a penalty to the loss function to prevent overfitting. It helps in simplifying the model by reducing large weights."
      ],
      "metadata": {
        "id": "0HHCQtKp0x3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain the difference between Lasso, Ridge, and Elastic Net regression\n",
        "- Lasso (L1): Adds absolute value of coefficients to the cost function; can shrink some coefficients to zero.\n",
        "\n",
        "- Ridge (L2): Adds squared values of coefficients to the cost; shrinks coefficients but doesn‚Äôt make them zero.\n",
        "\n",
        "- Elastic Net: Combines L1 and L2; useful when there are many correlated features."
      ],
      "metadata": {
        "id": "And7iQTY0yLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge.\n",
        "- When features are highly correlated or the dataset has more features than samples."
      ],
      "metadata": {
        "id": "HQ8chjsP0yeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (Œª) in Logistic Regression.\n",
        "- Larger Œª (or smaller C): More regularization, simpler model.\n",
        "\n",
        "- Smaller Œª (or larger C): Less regularization, more complex model."
      ],
      "metadata": {
        "id": "oogOIQBN0yuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression.\n",
        "- Linearity of independent variables and log-odds.\n",
        "\n",
        "- Independence of observations.\n",
        "\n",
        "- No multicollinearity.\n",
        "\n",
        "- Large sample size."
      ],
      "metadata": {
        "id": "_AKt3K_v0zKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks.\n",
        "- Decision Trees\n",
        "\n",
        "- Random Forest\n",
        "\n",
        "- SVM\n",
        "\n",
        "- KNN\n",
        "\n",
        "- Naive Bayes\n",
        "\n",
        "- Neural Networks"
      ],
      "metadata": {
        "id": "07LWSWDE0hJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics.\n",
        "- Accuracy\n",
        "\n",
        "- Precision\n",
        "\n",
        "- Recall\n",
        "\n",
        "- F1-Score\n",
        "\n",
        "- ROC-AUC\n",
        "\n",
        "- Confusion Matrix"
      ],
      "metadata": {
        "id": "1Vftqazl4Dbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression.\n",
        "- It can cause the model to be biased towards the majority class. Metrics like precision, recall, and F1-score become more informative in such cases."
      ],
      "metadata": {
        "id": "8EMXi2T_4GvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression.\n",
        "- It involves selecting the best parameters (e.g., C, solver, penalty) using techniques like GridSearchCV or RandomizedSearchCV."
      ],
      "metadata": {
        "id": "T_eIZXOO4G20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What are different solvers in Logistic Regression? Which one should be used.\n",
        "- liblinear: Good for small datasets, supports L1.\n",
        "\n",
        "- saga: Supports L1, L2, and elasticnet, scalable.\n",
        "\n",
        "- lbfgs: Good for L2 on multiclass problems.\n",
        "\n",
        "- newton-cg and sag: Good for L2 on large datasets."
      ],
      "metadata": {
        "id": "--rvZAh_4G94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification.\n",
        "- One-vs-Rest (OvR): Trains one binary classifier per class.\n",
        "\n",
        "- Softmax (Multinomial): Uses a single model that predicts all classes directly."
      ],
      "metadata": {
        "id": "RvLNY7Co4DhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression.\n",
        "- Advantages: Simple, interpretable, efficient.\n",
        "\n",
        "- Disadvantages: Poor performance on complex relationships, assumes linearity"
      ],
      "metadata": {
        "id": "ocOlCrvH6Alx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression.\n",
        "- Spam detection\n",
        "\n",
        "- Credit scoring\n",
        "\n",
        "- Medical diagnosis\n",
        "\n",
        "- Customer churn prediction"
      ],
      "metadata": {
        "id": "Be2Uguwb6A9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression.\n",
        "- Softmax is a generalization of Logistic Regression for multi-class classification. It outputs a probability distribution over classes."
      ],
      "metadata": {
        "id": "SXct30pf6BdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "- OvR: Better if classes are imbalanced.\n",
        "\n",
        "- Softmax: Better if classes are balanced and mutually exclusive."
      ],
      "metadata": {
        "id": "I2-NaL_R5-l8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "- Each coefficient represents the change in log-odds of the target per unit change in the feature."
      ],
      "metadata": {
        "id": "PE_9edRW5-6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "3qNNM09H5_DW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy\n",
        "- from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "param_dist = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", search.best_params_)\n",
        "print(\"Best Accuracy:\", search.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "J7nW0FwL5_a9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy\n",
        "- from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=1000))\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "LEt4w4HV6AGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "- from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm).plot()"
      ],
      "metadata": {
        "id": "sHLFCsge8o2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "- from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "hAAn3CRq8o-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'\n",
        "- model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy with class weights:\", model.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "R07WtHXD8pFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy.\n",
        "- import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "- Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
        "\n",
        "X = df[['Pclass', 'Sex', 'Age']]\n",
        "y = df['Survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Titanic Accuracy:\", accuracy_score(y_test, model.predict(X_test)))"
      ],
      "metadata": {
        "id": "DAuqpFsG8pMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy.\n",
        "- from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "- Without scaling\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "acc_no_scale = model.score(X_test, y_test)\n",
        "\n",
        "- With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = model_scaled.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_no_scale)\n",
        "print(\"Accuracy with scaling:\", acc_scaled)"
      ],
      "metadata": {
        "id": "0loFFRQj-waI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy.ÔøΩ\n",
        "- from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_prob = model_scaled.predict_proba(X_test_scaled)[:, 1]\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))"
      ],
      "metadata": {
        "id": "gI1R3NPZ-wqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy.\n",
        "- model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(\"Accuracy with C=0.5:\", model.score(X_test_scaled, y_test))"
      ],
      "metadata": {
        "id": "cFx4bM8V-wxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "- import numpy as np\n",
        "\n",
        "coefficients = model.coef_[0]\n",
        "feature_names = X.columns if isinstance(X, pd.DataFrame) else range(len(coefficients))\n",
        "for name, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{name}: {coef}\")"
      ],
      "metadata": {
        "id": "nen4SjlwAy3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification.\n",
        "- from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "print(\"Cohen‚Äôs Kappa Score:\", cohen_kappa_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "05kGAp1hAzMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score.\n",
        "- from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "43Sh6TzoAzUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance.\n",
        "- solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    print(f\"{solver} Accuracy:\", model.score(X_test_scaled, y_test))"
      ],
      "metadata": {
        "id": "7dZufc_IAzcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance.\n",
        "- from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9Od8UG8vB048"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling.\n",
        "- from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "- Load dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "- Train on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "acc_raw = model_raw.score(X_test, y_test)\n",
        "\n",
        "- Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "- Train on standardized data\n",
        "model_std = LogisticRegression(max_iter=1000)\n",
        "model_std.fit(X_train_std, y_train)\n",
        "acc_std = model_std.score(X_test_std, y_test)\n",
        "\n",
        "- Compare accuracies\n",
        "print(\"Accuracy on raw data: \", acc_raw)\n",
        "print(\"Accuracy on standardized data: \", acc_std)"
      ],
      "metadata": {
        "id": "84XHEjtIB1iO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "- from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "C_values = [0.01, 0.1, 1, 10]\n",
        "for c in C_values:\n",
        "    model = LogisticRegression(C=c, max_iter=1000)\n",
        "    scores = cross_val_score(model, X, y, cv=5)\n",
        "    print(f\"C={c}, CV Accuracy={np.mean(scores):.4f}\")"
      ],
      "metadata": {
        "id": "WW5n-XG1B2EC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy.\n",
        "- import joblib\n",
        "\n",
        "- Save\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "\n",
        "- Load and predict\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "print(\"Loaded model accuracy:\", loaded_model.score(X_test_scaled, y_test))"
      ],
      "metadata": {
        "id": "3SOcaVnjCDCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients.\n",
        "- from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "grid = GridSearchCV(model, param_grid, cv=5)\n",
        "grid.fit(X_train_std, y_train)\n",
        "\n",
        "print(\"Best C:\", grid.best_params_['C'])\n",
        "print(\"Best CV Accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "id": "dKcKLam5CDM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa\n",
        "Score.\n",
        "- import joblib\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "- Save the model\n",
        "joblib.dump(model_std, 'logistic_model.pkl')\n",
        "\n",
        "- Load the model\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "\n",
        "- Predict and evaluate\n",
        "y_pred = loaded_model.predict(X_test_std)\n",
        "print(\"Accuracy from loaded model:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "PTP6_o-ECDVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classification.\n",
        "- import joblib\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "- Save the model\n",
        "joblib.dump(model_std, 'logistic_model.pkl')\n",
        "\n",
        "- Load the model\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "\n",
        "- Predict and evaluate\n",
        "y_pred = loaded_model.predict(X_test_std)\n",
        "print(\"Accuracy from loaded model:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "pIvKmiwjCDd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy.\n",
        "- from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "- Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "- Model and parameter grid\n",
        "model = LogisticRegression(solver='saga', max_iter=5000)\n",
        "param_dist = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'solver': ['saga']\n",
        "}\n",
        "\n",
        "- RandomizedSearchCV\n",
        "search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=5, cv=3)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", search.best_params_)\n",
        "print(\"Best Accuracy:\", search.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "XgpkuyQaCDm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC).\n",
        "- from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "ovo_model.fit(X_train, y_train)\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "5GZOErjsCDwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling.\n",
        "- from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
      ],
      "metadata": {
        "id": "NyV6ShbtCD5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation.\n",
        "- from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "BmwJXBXMCEB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions.ÔøΩ\n",
        "- from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "CzzsyPvgB2sG"
      }
    }
  ]
}